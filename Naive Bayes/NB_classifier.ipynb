{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"dataset_NB.txt\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns dataframe containing the data from the dataset provided\n",
    "def dframe():\n",
    "    arraydata = [[]]\n",
    "    for line in data:\n",
    "        string = str(line)\n",
    "        result = int(line[-2:])\n",
    "        if (result == 0):\n",
    "          arraydata.append([string[:-3],0])\n",
    "        else:\n",
    "         arraydata.append([string[:-3],1])\n",
    "    data.close()\n",
    "    df = pd.DataFrame(arraydata, columns = ['Statement', 'Sentiment'])\n",
    "    df = df.dropna()\n",
    "    return df\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dframe()\n",
    "dataframe = dataframe.sample(frac=1, random_state=2)\n",
    "sevendframe = []\n",
    "training_data = []\n",
    "testing_data = []\n",
    "\n",
    "length = round(len(dataframe) * 1/7)\n",
    "\n",
    "for i in range(0,6,1):\n",
    "  sevendframe.append(  dataframe[i*length:(i+1)*length].reset_index(drop=True))\n",
    "sevendframe.append(dataframe[6*length:].reset_index(drop=True))\n",
    "\n",
    "for j in range(0,7):\n",
    "  testing_data.append(sevendframe[j])\n",
    "  train_data = []\n",
    "  for i in range(0,7): \n",
    "    if i!=j: \n",
    "      train_data.append(sevendframe[i])\n",
    "  training_data.append( pd.concat(train_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_test_set(message):\n",
    "   \n",
    "   p_spam_message = p_spam\n",
    "   p_ham_message = p_ham\n",
    "   message = re.sub('\\W', ' ', message) #message is the string parsed\n",
    "   message = message.lower().split() \n",
    "\n",
    "  \n",
    "\n",
    "   for word in message:\n",
    "      if word in parameters_spam:\n",
    "         p_spam_message *= parameters_spam[word]\n",
    "\n",
    "      if word in parameters_ham:\n",
    "         p_ham_message *= parameters_ham[word]\n",
    "\n",
    "   if p_ham_message > p_spam_message:\n",
    "      return 1\n",
    "   elif p_spam_message > p_ham_message:\n",
    "      return 0\n",
    "   else:\n",
    "      return 'cannot classify'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.61538461538461\n",
      "Accuracy: 81.81818181818181\n",
      "Accuracy: 82.51748251748252\n",
      "Accuracy: 84.61538461538461\n",
      "Accuracy: 83.91608391608392\n",
      "Accuracy: 75.52447552447552\n",
      "Accuracy: 83.80281690140845\n",
      "Average accuracy: 82.40140141548594\n"
     ]
    }
   ],
   "source": [
    "sumt=0\n",
    "\n",
    "for x in range(7):\n",
    "    training_set = training_data[x]\n",
    "    test_set = testing_data[x]\n",
    "    #removing punctuations from the training set\n",
    "    training_set['Statement'] = training_set['Statement'].str.lower()\n",
    "    training_set['Statement'] = training_set['Statement'].str.replace('the', ' ')\n",
    "    training_set['Statement'] = training_set['Statement'].str.replace(',', ' ')\n",
    "    training_set['Statement'] = training_set['Statement'].str.replace( '.', ' ')\n",
    "    training_set['Statement'] = training_set['Statement'].str.replace( '(', ' ')\n",
    "    training_set['Statement'] = training_set['Statement'].str.replace(')', ' ')\n",
    "    training_set['Statement'] = training_set['Statement'].str.replace('-', ' ')\n",
    "    training_set['Statement'] = training_set['Statement'].str.replace('!', ' ')\n",
    "    training_set['Statement'] = training_set['Statement'].str.replace('\\d+', ' ')\n",
    "    training_set['Statement'] = training_set['Statement'].str.split()\n",
    "\n",
    "    vocab = []\n",
    "    n_vocabulary = len(vocab)\n",
    "    \n",
    "    for statement in training_set['Statement']:\n",
    "        for word in statement:\n",
    "          vocab.append(word)\n",
    "\n",
    "    vocab = list(set(vocab))\n",
    "    wcps = {unique_word: [0] * len(training_set['Statement']) for unique_word in vocab}\n",
    "\n",
    "    for index, sms in enumerate(training_set['Statement']):\n",
    "        for word in sms:\n",
    "          wcps[word][index] += 1\n",
    "        \n",
    "        \n",
    "    word_counts = pd.DataFrame(wcps)\n",
    "    word_counts.reset_index(drop=True, inplace=True)\n",
    "    training_set.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    \n",
    "    training_set_clean = pd.concat([training_set, word_counts], axis=1)\n",
    "    spam_messages = training_set_clean[training_set_clean['Sentiment'] == 0]\n",
    "    ham_messages = training_set_clean[training_set_clean['Sentiment'] == 1]\n",
    "\n",
    "\n",
    "    p_spam = len(spam_messages) / len(training_set_clean) #probabilty of spam\n",
    "    p_ham = len(ham_messages) / len(training_set_clean)   #probability of not spam\n",
    "\n",
    "    noofwordsperspam = spam_messages['Statement'].apply(len)\n",
    "    n_spam = noofwordsperspam.sum()\n",
    "    \n",
    "\n",
    "    noofwordsperham = ham_messages['Statement'].apply(len)\n",
    "    n_ham = noofwordsperham.sum()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    laplacesmoothener = 1    #laplace smoothing\n",
    "    parameters_spam = {unique_word:0 for unique_word in vocab}\n",
    "    parameters_ham = {unique_word:0 for unique_word in vocab}\n",
    "\n",
    "#calculating parameters\n",
    "    for word in vocab:\n",
    "       noofwords_spam = spam_messages[word].sum() \n",
    "       p_word_given_spam = (noofwords_spam + laplacesmoothener) / (n_spam + laplacesmoothener*n_vocabulary)\n",
    "       parameters_spam[word] = p_word_given_spam\n",
    "\n",
    "       noofwords_ham = ham_messages[word].sum() \n",
    "       p_word_given_ham = (noofwords_ham + laplacesmoothener) / (n_ham + laplacesmoothener*n_vocabulary)\n",
    "       parameters_ham[word] = p_word_given_ham\n",
    "        \n",
    "        \n",
    "    test_set['predicted'] = test_set['Statement'].apply(classify_test_set)\n",
    "    matched = 0\n",
    "    total = test_set.shape[0]\n",
    "\n",
    "    for row in test_set.iterrows():\n",
    "       row = row[1]\n",
    "       if row['Sentiment'] == row['predicted']:\n",
    "          matched += 1\n",
    "    print('Accuracy:', matched*100/total)\n",
    "    sumt=sumt+(matched*100/total)\n",
    "print('Average accuracy:', sumt/7)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
